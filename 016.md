# 最小二乗法

<img src="https://github.com/murayama333/r2023/raw/main/docs/img/521.png" width="400px">

* 残差（予測値と実測値の誤差）の二乗和を最小化する方法
* データセットに最も適合する線形関数を見つけるために使用する
* たとえばモデルを $f(x) = a x +b$ とすると次のようになる

$$
E(a, b) = \sum_{i=1}^n (y_i - a x_i - b )^2
$$

* $y_i$ は目的変数の値（実測値）
* $x_i$ は説明変数の値
* $n$ はレコード数

> ここでx, yは定数である点に注意します。Eはa, bの関数です。より厳密には、E(a, b)は2変数の2次多項式関数です。

## まとめ

$$
a = \frac{\bar{xy} - \bar{x} \bar{y} }{\bar{x^2} -\bar{x}^2} = \frac{\sigma_{xy}}{\sigma_x^2}
$$

$$
b = -a\bar{x} + \bar{y}
$$

## 参考：解法

* a, bで偏微分した結果が0となる点を求める

$$
\frac{\partial E(a, b) }{ \partial a} = -2\sum_{i=1}^nx_i (y_i - a x_i - b) = 0
$$

$$
\frac{\partial E(a, b) }{ \partial b} = 2\sum_{i=1}^n (y_i - a x_i - b) = 0
$$

* 次のように整理できる

$$
a\sum_{i=1}^n x_i^2 + b \sum_{i=1}^n x_i = \sum_{i=1}^n x_i y_i \dots (1)
$$

$$
a\sum_{i=1}^n x_i + b n = \sum_{i=1}^n y_i  \dots (2)
$$

* (2)の両辺をnで割る

$$
a\frac{1}{n}\sum_{i=1}^n x_i + b = \frac{1}{n}\sum_{i=1}^n y_i
$$

$$
a\bar{x} + b = \bar{y}
$$

$$
b = -a\bar{x} + \bar{y}
$$

* これを(1)に代入する

$$
a\sum_{i=1}^n x_i^2 + (-a\bar{x} + \bar{y}) \sum_{i=1}^n x_i = \sum_{i=1}^n x_i y_i
$$

$$
a\sum_{i=1}^n x_i^2 -a\bar{x}\sum_{i=1}^n x_i + \bar{y}\sum_{i=1}^n x_i  = \sum_{i=1}^n x_i y_i
$$

$$
a(\sum_{i=1}^n x_i^2 -\bar{x}\sum_{i=1}^n x_i) + \bar{y}\sum_{i=1}^n x_i  = \sum_{i=1}^n x_i y_i  \dots (3)
$$

* (3)の両辺をnで割る

$$
a(\frac{1}{n} \sum_{i=1}^n x_i^2 -\bar{x} \frac{1}{n} \sum_{i=1}^n x_i) + \bar{y}\frac{1}{n}\sum_{i=1}^n x_i  = \frac{1}{n}\sum_{i=1}^n x_i y_i
$$

$$
a(\bar{x^2} -\bar{x}^2) + \bar{x} \bar{y} = \bar{xy}
$$

$$
a = \frac{\bar{xy} - \bar{x} \bar{y} }{\bar{x^2} -\bar{x}^2}
$$

* 分散と共分散を使って以下のように表現できる

$$
a = \frac{\sigma_{xy}}{\sigma_x^2}
$$

> 参考：説明変数が2つ以上になった場合、計算式が複雑になりますが連立方程式を解くという点は同じです。

---

## サンプルコード

```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import pandas as pd

iris = load_iris()
iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)
target = iris_data["sepal length (cm)"]
# data = iris_data[["sepal width (cm)",	"petal length (cm)",	"petal width (cm)"]]
data = iris_data[["sepal width (cm)"]]

x_train, x_test, y_train, y_test = train_test_split(data, target)

model = LinearRegression()
model.fit(x_train, y_train)
print("score:", model.score(x_test, y_test))
print("intercept:", model.intercept_)
print("coef:",  model.coef_)
```

### 実行結果

```
score: 0.01556118820839969
intercept: 6.429965919267973
coef: [-0.19673214]
```

---

## 参考：AIによる考察

残差の二乗和の最小化: 最小二乗法は、予測値と実測値の差（残差）の二乗和を最小化することにより、データセットに最も適合する線を見つける手法です。この方法は、誤差の大きさを強調し、正負の影響を相殺しないために二乗和を用いています。

外れ値への敏感性: 最小二乗法は外れ値に非常に敏感であり、これらの値は残差の二乗和に大きく影響し、モデルの精度を低下させる可能性があります。データの前処理で外れ値を取り除くか、別の手法を検討することが重要です。

非線形関係への対応: 最小二乗法は線形モデルに最適ですが、非線形関係を持つデータには適していません。非線形データに対応するためには、多項式回帰や他の非線形モデルを使用することが望ましいです。
