# 機械学習の評価指標 - 分類問題
* 分類問題では目的変数が質的データ（カテゴリカルデータ）となる
* 分類結果と実測値が一致するかどうかを調べることができる（正答率）
* 正答率以外にも適合率や再現率、F1値といった評価指標がある

## 正答率 (Accuracy)

* 全データポイントのうち、正しく予測されたポイントの割合
* 正の予測と負の予測の両方を含む全体的な正確さを測定
* 不均衡なクラス分布では誤解を招く可能性がある

---

## 適合率 (Precision)

* 予測が真であるケースのうち、実際に真であったケースの割合
* 偽陽性（誤って真と予測）を低減することに焦点を当てる
* 「精度」とも表され、予測の正確さを測定する

### 例：ゲームの勝ち負け

||予測：勝ち|予測：負け|
|:--|:--|:--|
|実際：勝ち|50|20|
|実際：負け|10|20|

$$
Precision = \frac{50}{50 + 10} = 0.833
$$

> 大きなリスクを伴う意思決定など、失敗できないケースでは適合率が重要となります。たとえばクレジットカードなどの不正利用を検出する場合は適合率が重要になります。

---

## 再現率 (Recall)

* 実際に真であるケースのうち、予測が真であったケースの割合
* 偽陰性（誤って偽と予測）を低減することに焦点を当てる
* 「感度」とも呼ばれ、クラスの完全な捉え方を測定する

### 例：ゲームの勝ち負け

||予測：勝ち|予測：負け|
|:--|:--|:--|
|実際：勝ち|50|20|
|実際：負け|10|20|

$$
Precision = \frac{50}{50 + 20} = 0.714
$$

> たとえば、疾患の診断においては、偽陰性（実際には疾患があるにもかかわらず、疾患がないと誤って診断されるケース）を低減することが重要です。このような場合は再現率に着目して評価します。

---

## F1スコア

* 適合率（Precision）と再現率（Recall）の両方のバランスを加味した指標
* 適合率と再現率の調和平均を求める
* 不均衡なクラス分布を持つデータセットにおいて特に有用となる

$$
F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
$$

### 参考：調和平均

* 平均を求める方法の一つ
* 各数値が単位あたりの量を表す場合に有用
* 数値の逆数の算術平均の、逆数として計算される

$$
H = \frac{n}{1/x_1 + 1/x_2 + \cdots + 1/x_n}
$$

---

## サンプルコード

```py
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

iris = load_iris()
iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)
iris_data["target"] = iris.target
# 0(stosa): 50, 1(versicolor): 50
iris_100 = iris_data.loc[iris_data["target"] != 2, :]
target = iris_100["target"]
data = iris_100[["sepal length (cm)"]]

x_train, x_test, y_train, y_test = train_test_split(data, target, random_state=0)
model = DecisionTreeClassifier(max_depth=1, random_state=0)
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

# 正答率
accuracy = accuracy_score(y_test, y_pred)
print("accuracy:", accuracy)

# 適合率
precision = precision_score(y_test, y_pred)
print("precision:", precision)

# 再現率
recall = recall_score(y_test, y_pred)
print("recall:", recall)

# F値
f1 = f1_score(y_test, y_pred)
print("f1:" ,f1)

# レポート
# report = classification_report(y_test, y_pred, output_dict=True)
# pd.DataFrame(report)
```
