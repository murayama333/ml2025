# 過学習

* モデルが訓練データに最適化されてしまうことで汎化性能を失う現象のこと
* 過学習したモデルは訓練データに対しては高い精度を示すが、未知のデータに対しては精度が低下する
* 過度に複雑なモデルを作成する場合や訓練データが不足する場合など様々な要因で発生する

> 過学習を防ぐには、モデルの正則化やデータセットの拡張、クロスバリデーションなど様々な手法があります。

## サンプルコードの説明

* irisデータで回帰分析について学習する

* 目的変数
  + sepal length (cm)
* 説明変数
  + sepal width (cm)
  + petal length (cm)
  + petal width (cm)

* 回帰式

$$
f(x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3 
$$

```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import pandas as pd

iris = load_iris()
iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)
target = iris_data["sepal length (cm)"]
data = iris_data[["sepal width (cm)",	"petal length (cm)",	"petal width (cm)"]]

x_train, x_test, y_train, y_test = train_test_split(data, target, train_size=0.7, random_state=1)

model = LinearRegression()
model.fit(x_train, y_train)
print("train score:", model.score(x_train, y_train))
print("test score:", model.score(x_test, y_test))
print("intercept:", model.intercept_)
print("coef:",  model.coef_)
```

### 実行結果

```
train score: 0.8752244005398927
test score: 0.7954806516573668
intercept: 1.9279315401986938
coef: [ 0.60171048  0.78387971 -0.74440992]
```

---

## サンプルコード（2次の多項式回帰）

* PolynomialFeaturesによって多項式回帰モデルを作成できる

* 回帰式

$$
f(x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_1^2 + w_5 x_1 x_2 + w_6 x_1 x_3 + w_7 x_2^2 + w_8 x_2 x_3 + w_9 x_3^2
$$

> PolynomialFeaturesではx1x2のような交差項も作成できる点に注意してください。

```py
# 2次の多項式特徴を生成
from sklearn.preprocessing import PolynomialFeatures

iris = load_iris()
iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)
target = iris_data["sepal length (cm)"]
data = iris_data[["sepal width (cm)",	"petal length (cm)",	"petal width (cm)"]]

x_train, x_test, y_train, y_test = train_test_split(data, target, train_size=0.7, random_state=1)

poly = PolynomialFeatures(degree=2)
x_train_poly = poly.fit_transform(x_train)
x_test_poly = poly.transform(x_test)

model = LinearRegression()
model.fit(x_train_poly, y_train)

print("train score:", model.score(x_train_poly, y_train))
print("test score:", model.score(x_test_poly, y_test))
print("intercept:", model.intercept_)
print("coef:",  model.coef_)
```

> 特徴量が $n$ 個のとき、degree を $a$ とすると、（=「次数 $a$ 以下」の多項式特徴を作ると）
>
> * 作られる特徴量の個数（**bias=1 も含む**）は $\displaystyle \binom{n+a}{a}$
> * `include_bias=False` のときは上記から **1（定数項）を引く**ので、$\displaystyle \binom{n+a}{a}-1$
>
> 例（この資料の設定）:
>
> * 元の特徴量は 3 個（$n=3$）
> * 2次（$a=2$）で `include_bias=True`（デフォルト）なら $\binom{3+2}{2}=\binom{5}{2}=10$ 個（実行結果の `coef` も 10 個）
> * 3次（$a=3$）で `include_bias=False` なら $\binom{3+3}{3}-1=\binom{6}{3}-1=20-1=19$ 個（後半の `coef` が 19 個）

### 実行結果

```
train score: 0.8824279077898773
test score: 0.8083119846173312
intercept: 1.9341455936795509
coef: [ 0.          0.565096    1.36032475 -2.39779132  0.04372248 -0.25483221
  0.5485142  -0.05140246  0.43435296 -0.60814906]
```

> 多項式回帰の次数を3, 4, 5と上げて複雑なモデルを生成します。

### 補足

> ここでのサンプルコードはデータ件数も少ないため結果はやや不安定です。


## 多項式回帰と標準化

* 多項式特徴（例: $x^3$, $x_1x_2$ ）は元の特徴量より値の桁や分散が大きくなりやすい
* スケール差が大きいと、期待通りに係数を推定できない可能性がある
* 特徴量を標準化（平均0・分散1）しておくと係数の大小も比較しやすくなる（特に Ridge/Lasso など正則化を使う場合に重要）

```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression
import pandas as pd

iris = load_iris()
iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)
target = iris_data["sepal length (cm)"]
data = iris_data[["sepal width (cm)", "petal length (cm)", "petal width (cm)"]]

x_train, x_test, y_train, y_test = train_test_split(
    data, target, train_size=0.7, random_state=1
)

# 多項式特徴
poly = PolynomialFeatures(degree=3)
x_train_poly = poly.fit_transform(x_train)
x_test_poly = poly.transform(x_test)

# 標準化
scaler = StandardScaler()
x_train_poly_std = scaler.fit_transform(x_train_poly)
x_test_poly_std = scaler.transform(x_test_poly)

model = LinearRegression()
model.fit(x_train_poly_std, y_train)

print("train score:", model.score(x_train_poly_std, y_train))
print("test score:", model.score(x_test_poly_std, y_test))
print("intercept:", model.intercept_)
print("coef:", model.coef_)
```

### 実行結果

```
train score: 0.9048385289524471
test score: 0.7456622744661515
intercept: 5.799999999999962
coef: [ -2.43181536   4.7850564   -7.76572596   4.84835255  -6.43110607
  10.99714116   8.20869098 -14.49518047   9.87959797  -2.39685251
   4.01725209  -4.45877367  -7.62684129  12.68887494  -9.46801001
 -23.81558673  72.39450767 -76.46254181  28.33033284]
```


### Pipelineによる実装

* `Pipeline` は前処理から学習器の設定を1つのモデルとしてまとめる
* `fit` / `predict` / `score` を一括で呼べるようにする
* 処理の流れをシンプルに記述できる

```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
import pandas as pd

iris = load_iris()
iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)
target = iris_data["sepal length (cm)"]
data = iris_data[["sepal width (cm)", "petal length (cm)", "petal width (cm)"]]

x_train, x_test, y_train, y_test = train_test_split(
    data, target, train_size=0.7, random_state=1
)

# Pipeline（Polynomial -> StandardScaler -> LinearRegression）
model = Pipeline([
    ("poly", PolynomialFeatures(degree=3)),
    ("scaler", StandardScaler()),
    ("lr", LinearRegression())
])

model.fit(x_train, y_train)

print("train score:", model.score(x_train, y_train))
print("test score:", model.score(x_test, y_test))

# intercept / coef は名前を指定して取得
print("intercept:", model.named_steps["lr"].intercept_)
print("coef:", model.named_steps["lr"].coef_)
```

### 実行結果

```
train score: 0.9048385289524471
test score: 0.7456622744661515
intercept: 5.799999999999962
coef: [ -2.43181536   4.7850564   -7.76572596   4.84835255  -6.43110607
  10.99714116   8.20869098 -14.49518047   9.87959797  -2.39685251
   4.01725209  -4.45877367  -7.62684129  12.68887494  -9.46801001
 -23.81558673  72.39450767 -76.46254181  28.33033284]
```