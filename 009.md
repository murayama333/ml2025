# 機械学習 - 特徴量エンジニアリング

## データの準備

* sklearnで利用可能なカリフォルニア住宅価格データセットを使う

```python
from sklearn.datasets import fetch_california_housing
california_housing = fetch_california_housing()
california_housing
```

* 特徴量
  * MedInc: 地域の中央所得
  * HouseAge: 住宅の平均年齢
  * AveRooms: 1住戸あたりの平均部屋数
  * AveBedrms: 1住戸あたりの平均寝室数
  * Population: 地域の人口
  * AveOccup: 1住戸あたりの平均居住者数
  * Latitude: 緯度
  * Longitude: 経度
* 目的変数
  * Price: 住宅価格の中央値

* pandasのDataFrameに置き換える

```python
import pandas as pd
features = pd.DataFrame(california_housing.data,
                          columns=california_housing.feature_names)
target = pd.DataFrame(california_housing.target, columns=["Price"])
housing = pd.concat([features, target], axis=1)
housing
```

* 後の処理で必要になるパッケージをimportしておく

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import numpy as np
```

## 線形回帰モデル - 単回帰分析

### 1. 特徴量を選択する

* 特徴量について考察する

```py
housing.describe()
```

* 各特徴量と目的変数の散布図を確認する

```py
plt.scatter(housing["MedInc"], housing["Price"])
```

* 各特徴量と目的変数の相関係数を確認する

```py
# 相関係数を確認する
housing.corr()
```

* 外れ値を処理する

```py
plt.scatter(housing["AveOccup"], housing["Price"])
```

* `AveOccup` には外れ値がいくつか存在するので削除しておく

```py
housing = housing[housing["AveOccup"] <= 10]
```

> 非線形な関係性など相関係数を見るだけでは気づけないこともあります。相関係数だけでなく散布図を確認するようにします。

### 2. データを分割する

```py
x = housing.loc[:, ["MedInc"]]
y = housing.loc[:, "Price"]
x_train, x_test, y_train, y_test = train_test_split(x, y)
```

> ここでは仮に "MedInc" 列を選択したものとします。

### 3. 学習する

```py
model = LinearRegression()
model.fit(x_train, y_train)
```

### 4. モデルを評価する

```py
y_pred = model.predict(x_test)
print("MSE", mean_squared_error(y_test, y_pred))
print("R2", r2_score(y_test, y_pred))
```

### 特徴量エンジニアリング - 多項式回帰

* MedIncの2乗値、3乗値を追加する

```py
housing['MedInc2'] = housing['MedInc']**2
housing['MedInc3'] = housing['MedInc']**3
```

* データを分割する

```
x = housing.loc[:, ["MedInc", "MedInc2", "MedInc3"]]
y = housing.loc[:, "Price"]
x_train, x_test, y_train, y_test = train_test_split(x, y)
```

> あとは学習と評価を再実行します。

### 参考：回帰式の可視化

```py
plt.scatter(housing["MedInc"], housing["Price"])
range = np.arange(0, housing["MedInc"].max(), 0.1)
sample = pd.DataFrame({"MedInc": range, "MedInc2": range**2, "MedInc3": range**3})
plt.plot(range, model.predict(sample), color="red")
```

---

### 1. 特徴量を選択する

* 散布図や相関係数を参考にして特徴量を選択する

> 各特徴量と目的変数の相関だけでなく、特徴量間の相関にも注意する必要があります（多重共線性の問題）。

### 2. データを分割する

```py
x = housing.loc[:, ["MedInc", "HouseAge", "AveRooms"]]
y = housing.loc[:, "Price"]
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)
```

> ここでは仮に "MedInc", "HouseAge", "AveRooms" の3列を選択したもとのします。

### 3. 学習する

```py
model = LinearRegression()
model.fit(x_train,  y_train)
y_pred = model.predict(x_test)
```

### 4. 学習済みモデルを評価する

```py
print("MSE", mean_squared_error(y_test, y_pred))
print("R2", r2_score(y_test, y_pred))
```

### 特徴量エンジニアリング - 交互作用特徴量

* 交互作用特徴量とは、複数の特徴量からの演算によって新たな特徴量を定義すること
* たとえば `AveBedrms / AveRooms` を計算すれば、ベッドルーム率を計算できる

```py
housing["BedRatio"] = housing["AveBedrms"] / housing["AveRooms"]
housing
```

---

## 参考：データの標準化

* 平均:0、標準偏差:1のデータに変換んする（z変換）

```py
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_housing = scaler.fit_transform(housing)
sc_housing = pd.DataFrame(scaled_housing, columns=housing.columns)
sc_housing.describe()
```

* 変換後のデータを使って学習、評価する

```py
x = housing.loc[:, ["MedInc", "HouseAge", "AveRooms"]]
y = housing.loc[:, "Price"]
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)
model.fit(x_train,  y_train)
y_pred = model.predict(x_test)
print("MSE", mean_squared_error(y_test, y_pred))
print("R2", r2_score(y_test, y_pred))
print("coef", model.coef_)
```

> データを標準化することで回帰係数を比較しやすくなります。

---

## 参考：訓練データ、検証データ、テストデータの分割

* テストデータへの最適化を防ぐために、教師データを以下の3つに分割して学習を進める
  * 訓練データ(x_train, y_train)
  * 検証データ(x_valid, y_valid)
  * テストデータ(x_test, y_test)
