# アンサンブル学習
* 複数の学習アルゴリズムを組み合わせて、単一のモデルよりも優れた予測性能を目指す
* 過学習を減少させ、さまざまなデータセットに対するモデルの堅牢性と精度を向上させる
* 代表的なアンサンブル学習の手法にはバギング、ブースティング、スタッキングなどがある

## バギング

* 元の訓練データセットからランダムにサンプリング（ブートストラップ）して複数のサブセットを作成し、それぞれでモデルを訓練する手法
* 各サブセットで訓練された複数のモデル（決定木など）の予測を平均化または多数決によって結合し、最終的な予測を行う
* 個々のモデルの過学習を減少させ、データのランダムな変動に対する堅牢性を向上させる

> バギングの語源はBootstrap aggregating（ブートストラップ集約）です。ここでのBootstrapとはランダムにサンプリング（復元抽出）することです。

```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

iris = load_iris()
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)

model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=0)
model.fit(x_train, y_train)
print("acc:", model.score(x_test, y_test))
```

### 実行結果

```
acc: 0.9736842105263158
```

### ランダムフォレストとバギング

* ランダムフォレストはバギングの概念に加えて、特徴量もランダムに取得している

---

## ブースティング

* 複数の弱学習器を逐次的に訓練し、それらを組み合わせて強力な予測モデルを構築する
* 後続の学習器は、前の学習器が誤分類したデータポイントに大きな重みを付けて訓練される
* ブースティングは予測精度を大幅に向上させることができるが、過学習のリスクも高まる可能性がある

> 代表的なブースティングアルゴリズムには、AdaBoost、Gradient Boosting、XGBoostなどがあります。

```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)

base_model = DecisionTreeClassifier(max_depth=3, random_state=0)
model = AdaBoostClassifier(n_estimators=50, estimator=base_model, random_state=0)

model.fit(x_train, y_train)
print("acc:", model.score(x_test, y_test))
```

### 実行結果

```py
acc: 0.9736842105263158
```

### ブースティングと過学習

* 連続的に誤りを修正しながら学習を進める過程で、モデルは訓練データに対して非常に強く適合し、ノイズや外れ値にも適合してしまう可能性がある

> 一般的に複雑なモデルを構築することは過学習につながるリスクがあります。

---

## 参考：スタッキング

* 異なる種類の複数の学習モデルを組み合わせて、それぞれの予測結果を入力とする新しいモデル（メタモデル）を訓練する手法
* 最初のレイヤーで複数の基本モデルが訓練され、次のレイヤーでこれらのモデルの出力を組み合わせて最終的な予測を行うメタモデルが訓練される
* 異なるモデルの予測を統合することで、個々のモデルよりも高い予測精度を達成することが可能

> スタッキングでは適切なモデルの選択とパラメータ調整が重要となります。

```py
from sklearn.datasets import load_iris
from sklearn.ensemble import StackingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

iris = load_iris()
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)

base_estimators = [
    ('dt', DecisionTreeClassifier()),
    ("knn", KNeighborsClassifier())
]

model = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())

model.fit(x_train, y_train)
print("acc:", model.score(x_test, y_test))
```

#### 実行結果

```py
acc: 0.9736842105263158
```
