# 正則化
* 誤差関数E(w) に正則化項R(w)を設定することで過学習を防ぐ
* 一般的な、正則化項R(w)にはL1正則化、L2正則化の2つが用いられる

$$
E'(w) = E(w) + \lambda R(w)
$$

> λはハイパーパラメータ（λ > 0）

## まとめ

### L2正則化（リッジ回帰）

* 重みの二乗和（L2ノルム）を最小化し、パラメータの値を小さく保つことによって過学習を防ぐ

$$
R(w) = \sum_{i=1}^n w_i ^2
$$

### L1正則化（ラッソ回帰）

* L1正則化は、重みの絶対値の和（L1ノルム）を最小化し、いくつかの重みをゼロにすることで、より単純なモデルを作成する

$$
R(w) = \sum_{i=1}^n |w_i|
$$

---

## L2正則化（リッジ回帰）

```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
import pandas as pd

iris = load_iris()
iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)
target = iris_data["sepal length (cm)"]
data = iris_data[["sepal width (cm)",	"petal length (cm)",	"petal width (cm)"]]

x_train, x_test, y_train, y_test = train_test_split(data, target)

model = Ridge()
model.fit(x_train, y_train)
print("score:", model.score(x_test, y_test))
print("intercept:", model.intercept_)
print("coef:",  model.coef_)
```

### 実行結果

```
score: 0.8425459785313563
intercept: 2.2835270686721474
coef: [ 0.55123177  0.60144126 -0.3367802 ]
```

---

## L1正則化（ラッソ回帰）

```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
import pandas as pd

iris = load_iris()
iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)
target = iris_data["sepal length (cm)"]
data = iris_data[["sepal width (cm)",	"petal length (cm)",	"petal width (cm)"]]

x_train, x_test, y_train, y_test = train_test_split(data, target)

model = Lasso(alpha=1)
model.fit(x_train, y_train)
print("score:", model.score(x_test, y_test))
print("intercept:", model.intercept_)
print("coef:",  model.coef_)
```

### 実行結果

```
score: 0.3334595464330281
intercept: 5.434247611735829
coef: [-0.          0.10717764  0.        ]
```

---
