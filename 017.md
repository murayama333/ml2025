# 正則化

* 誤差関数E(w) に正則化項R(w)を設定することで過学習を防ぐ
* モデルの複雑さ（重みの大きさ）にペナルティを与える
* 一般的に正則化項R(w)には、L1正則化、L2正則化の2つが用いられる

$$
E'(w) = E(w) + \lambda R(w)
$$

> λはハイパーパラメータ（λ > 0）。λが大きいほど → ペナルティは大きくなり、シンプルなモデルになります。

## 参考：誤差関数（最小二乗法）

* 線形回帰モデルで使う誤差関数 $E(w)$ は以下のとおり

$$
E(w) = \sum_{i=1}^{N} (y_i - \hat{y_i})^2
$$

$$
\hat{y} = w x
$$

> $w$ は重みベクトル、$x$ は特徴量、 $\hat{y}$ はモデルの推論した値です。

## L2正則化（リッジ回帰）

* 重みの二乗和（L2ノルム）を最小化し、パラメータの値を小さく保つことによって過学習を防ぐ

$$
R(w) = \sum_{i=1}^n w_i ^2
$$

### L2正則化（リッジ回帰）のサンプルコード

```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
import pandas as pd

iris = load_iris()
iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)
target = iris_data["sepal length (cm)"]
data = iris_data[["sepal width (cm)", "petal length (cm)", "petal width (cm)"]]

x_train, x_test, y_train, y_test = train_test_split(data, target, train_size=0.7, random_state=1)

model = Ridge(alpha=1)
model.fit(x_train, y_train)
print("train score:", model.score(x_train, y_train))
print("test score:", model.score(x_test, y_test))
print("intercept:", model.intercept_)
print("coef:",  model.coef_)
```

> scikit-learn では、正則化の強さ $\lambda$ は `alpha` という引数で指定します。

### 実行結果

```txt
train score: 0.8723758938332707
test score: 0.8043603905979594
intercept: 2.227389466512192
coef: [ 0.53877455  0.68851967 -0.53591155]
```

> リッジ回帰の場合は、係数が小さくなります。

## L1正則化（ラッソ回帰）

* L1正則化は、重みの絶対値の和（L1ノルム）を最小化し、いくつかの重みをゼロにすることで、より単純なモデルを作成する

$$
R(w) = \sum_{i=1}^n |w_i|
$$

### L1正則化（ラッソ回帰）のサンプルコード

```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
import pandas as pd

iris = load_iris()
iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)
target = iris_data["sepal length (cm)"]
data = iris_data[["sepal width (cm)", "petal length (cm)", "petal width (cm)"]]

x_train, x_test, y_train, y_test = train_test_split(data, target, train_size=0.7, random_state=1)

model = Lasso(alpha=1)
model.fit(x_train, y_train)
print("train score:", model.score(x_train, y_train))
print("test score:", model.score(x_test, y_test))
print("intercept:", model.intercept_)
print("coef:",  model.coef_)
```

### 実行結果

```txt
train score: 0.34040493808741146
test score: 0.2969821034685235
intercept: 5.41375098750873
coef: [-0.          0.10332776  0.        ]
```

> ラッソ回帰の場合は、寄与しない係数の値が0になります。また今回は alpha=1 と強めの正則化をかけているため、モデルが単純になりすぎ、精度が大きく低下しています。

---

### 補足

* Lasso回帰、Ridge回帰は特徴量の単位の影響を受けるので、事前に標準化することが好ましい
* Irisデータの場合は、特徴量の単位がcmで統一されている

* Ridge = 山の背、尾根という意味
  * 重みが大きく変動せずになだらかになるイメージ

* LASSO = Least Absolute Shrinkage and Selection Operator
  * Least Absolute：絶対値を使った
  * Shrinkage：縮小
  * Selection：選択
  * Operator：手法
