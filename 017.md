# 正則化

* 誤差関数 $E(w)$ に正則化項 $R(w)$ を設定することで過学習を防ぐ
* モデルの複雑さ（重みの大きさ）にペナルティを与える
* 一般的に正則化項R(w)には、L1正則化、L2正則化の2つが用いられる

$$
J(w) = E(w) + \lambda R(w)
$$

> $\lambda$ はハイパーパラメータ（$\lambda > 0$）。$\lambda$ が大きいほどペナルティは大きくなり、シンプルなモデルになります。

## 参考：誤差関数（最小二乗法）

* 線形回帰モデルで使う誤差関数 $E(w)$ は以下のとおり

$$
E(w) = \sum_{i=1}^{N} (y_i - \hat{y_i})^2
$$

$$
\hat{y} = w^\top x + b
$$

> $w$ は重みベクトル、$x$ は特徴量ベクトル、$b$ は切片、$\hat{y}$ はモデルの推論した値です（$w^\top x$ は内積）。

## L2正則化（リッジ回帰）

* 重みの二乗和（L2ノルム）を最小化し、パラメータの値を小さく保つことによって過学習を防ぐ

$$
R(w) = \sum_{i=1}^n w_i ^2
$$

### L2正則化（リッジ回帰）のサンプルコード

```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
import pandas as pd

iris = load_iris()
iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)
target = iris_data["sepal length (cm)"]
data = iris_data[["sepal width (cm)", "petal length (cm)", "petal width (cm)"]]

x_train, x_test, y_train, y_test = train_test_split(data, target, train_size=0.7, random_state=1)

model = Ridge(alpha=1)
model.fit(x_train, y_train)
print("train score:", model.score(x_train, y_train))
print("test score:", model.score(x_test, y_test))
print("intercept:", model.intercept_)
print("coef:",  model.coef_)
```

> scikit-learn では、正則化の強さを表す係数を `alpha` という引数で指定します。

### 実行結果

```txt
train score: 0.8723758938332707
test score: 0.8043603905979594
intercept: 2.227389466512192
coef: [ 0.53877455  0.68851967 -0.53591155]
```

> リッジ回帰の場合は、係数が小さくなります。


### 複雑なモデル（多項式回帰）の例

```py
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    data, target, train_size=0.7, random_state=1
)

model = Pipeline([
    ("poly", PolynomialFeatures(degree=5, include_bias=False)),
    ("scaler", StandardScaler()),
    ("ridge", Ridge(alpha=1.0))
])

model.fit(x_train, y_train)
print("train score:", model.score(x_train, y_train))
print("test score:", model.score(x_test, y_test))
print("intercept:", model.named_steps["ridge"].intercept_)
print("coef:", model.named_steps["ridge"].coef_ )
```

---

## L1正則化（ラッソ回帰）

* L1正則化は、重みの絶対値の和（L1ノルム）を最小化し、いくつかの重みをゼロにすることで、より単純なモデルを作成する

$$
R(w) = \sum_{i=1}^n |w_i|
$$

### L1正則化（ラッソ回帰）のサンプルコード

```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
import pandas as pd

iris = load_iris()
iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)
target = iris_data["sepal length (cm)"]
data = iris_data[["sepal width (cm)", "petal length (cm)", "petal width (cm)"]]

x_train, x_test, y_train, y_test = train_test_split(data, target, train_size=0.7, random_state=1)

model = Lasso(alpha=1)
model.fit(x_train, y_train)
print("train score:", model.score(x_train, y_train))
print("test score:", model.score(x_test, y_test))
print("intercept:", model.intercept_)
print("coef:",  model.coef_)
```

### 実行結果

```txt
train score: 0.34040493808741146
test score: 0.2969821034685235
intercept: 5.41375098750873
coef: [-0.          0.10332776  0.        ]
```

> ラッソ回帰の場合は、寄与しない係数の値が0になります。また今回は alpha=1 と強めの正則化をかけているため、モデルが単純になりすぎ、精度が大きく低下しています。


### 複雑なモデル（多項式回帰）の例

```py
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    data, target, train_size=0.7, random_state=1
)

model = Pipeline([
    ("poly", PolynomialFeatures(degree=5, include_bias=False)),
    ("scaler", StandardScaler()),
    ("lasso", Lasso(alpha=0.01)) # alphaを調整するのがポイント
 ])

model.fit(x_train, y_train)
print("train score:", model.score(x_train, y_train))
print("test score:", model.score(x_test, y_test))
print("intercept:", model.named_steps["lasso"].intercept_)
print("coef:", model.named_steps["lasso"].coef_ )
```

---

### 補足

* Lasso回帰、Ridge回帰は特徴量の単位の影響を受けるので、事前に標準化することが好ましい
* Ridge = 山の背、尾根という意味
  * 重みが大きく変動せずになだらかになるイメージ
* Lasso = Least Absolute Shrinkage and Selection Operator
  * Least Absolute：絶対値を使った
  * Shrinkage：縮小
  * Selection：選択
  * Operator：手法